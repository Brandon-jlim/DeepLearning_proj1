{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f5f898",
   "metadata": {
    "id": "77f5f898"
   },
   "source": [
    "## Requirements\n",
    "```\n",
    "Python                  3.9.7\n",
    "\n",
    "clip                    1.0\n",
    "lightgbm                3.2.1\n",
    "numpy                   1.21.2\n",
    "opencv-python           4.5.3.56\n",
    "pandas                  1.3.3\n",
    "scipy                   1.7.1\n",
    "sklearn                 1.0\n",
    "torch                   1.9.1\n",
    "torchvision             0.10.1\n",
    "tqdm                    4.62.3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca0de9e",
   "metadata": {
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1634458674495,
     "user": {
      "displayName": "Jaesun Park",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgZoXwLcDlJDxjDM_dDPkJjpEemuLU0RkMAa3ImbQU=s64",
      "userId": "10621921894899269106"
     },
     "user_tz": -540
    },
    "id": "cca0de9e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from itertools import combinations\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine, pdist\n",
    "\n",
    "from PIL import Image\n",
    "import clip\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets.folder import pil_loader\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0cf174",
   "metadata": {
    "id": "cb0cf174"
   },
   "source": [
    "## Json 파일 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92145c05",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "- Blob detector를 활용하여 이미지 내 붉은 점의 개수를 파악하여 feature로 활용\n",
    "- 각 keypoint간의 거리, 각도, 차이벡터 등을 적절히 normalize하여 feature로 활용\n",
    "- 위 feature를 활용하여 손가락을 접었는지, 이미지의 손이 몇개인지 discrete feature를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28e89638",
   "metadata": {
    "executionInfo": {
     "elapsed": 963,
     "status": "ok",
     "timestamp": 1634458677788,
     "user": {
      "displayName": "Jaesun Park",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgZoXwLcDlJDxjDM_dDPkJjpEemuLU0RkMAa3ImbQU=s64",
      "userId": "10621921894899269106"
     },
     "user_tz": -540
    },
    "id": "28e89638"
   },
   "outputs": [],
   "source": [
    "def make_blob_detector():\n",
    "    \"\"\"\n",
    "    json 파일에 기록된 keypoint의 개수는 이미지 내 keypoint의 수와 일치하지 않음\n",
    "    json의 keypoint가 21개임에도 양손인 경우, 42개임에도 한손인 경우가 있음\n",
    "    하지만 그 경우에도 이미지 내에는 알맞은 개수의 붉은색 keypoint가 표시되어 있음\n",
    "    이를 파악하기 위해 OpenCV의 Blob Detector를 활용\n",
    "\n",
    "    적절한 옵션의 Blob detector를 생성\n",
    "    \"\"\"\n",
    "    params = cv2.SimpleBlobDetector_Params()\n",
    "    params.filterByColor = True\n",
    "    params.blobColor = 255\n",
    "    params.minThreshold = 253\n",
    "    params.maxThreshold = 255\n",
    "    params.thresholdStep = 1\n",
    "    params.minDistBetweenBlobs = 0\n",
    "    params.filterByArea = True\n",
    "    params.maxArea = 1000\n",
    "    params.filterByConvexity = False\n",
    "    params.filterByConvexity = False\n",
    "    params.filterByInertia = False\n",
    "    return cv2.SimpleBlobDetector_create(params)\n",
    "\n",
    "\n",
    "def get_num_keypoint(image_path, detector):\n",
    "    \"\"\"\n",
    "    Blob detector를 이용하여 image에서 붉은색 점의 개수를 반환\n",
    "    \"\"\"\n",
    "    im = cv2.imread(image_path)\n",
    "    im_tf = (im == [0, 0, 255]).all(-1, keepdims=True)\n",
    "    keypoints = detector.detect(im_tf.astype(np.uint8)*255)\n",
    "    return len(keypoints)\n",
    "\n",
    "\n",
    "def included_angle_cos_dist(v1, v2, v3):\n",
    "    return cosine(v1 - v2, v3 - v2)\n",
    "\n",
    "def cos(vec1, vec2):\n",
    "    \"\"\"\n",
    "    vec1과 vec2 사잇각에 대한 cosine\n",
    "    \"\"\"\n",
    "    return np.dot(vec1, vec2) / np.linalg.norm(vec1) / np.linalg.norm(vec2)\n",
    "\n",
    "def cos_vec3(v1, v2, v3):\n",
    "    vec1, vec2 = v1 - v2, v3 - v2\n",
    "    return cos(vec1, vec2)\n",
    "\n",
    "def clockwise_sin2d(vec1, vec2):\n",
    "    \"\"\"\n",
    "    vec1과 vec2 시계 방향의 각도에 대한 sine\n",
    "    아래 참고\n",
    "    https://stackoverflow.com/a/16544330\n",
    "    https://stackoverflow.com/a/53970746\n",
    "    \"\"\"\n",
    "    return (vec1[0] * vec2[1] - vec1[1] * vec2[0]) / np.linalg.norm(vec1) / np.linalg.norm(vec2)\n",
    "\n",
    "def clockwise_sin2d_vec3(v1, v2, v3):\n",
    "    vec1, vec2 = v1 - v2, v3 - v2\n",
    "    return (clockwise_sin2d(vec1, vec2))\n",
    "\n",
    "\n",
    "# 끼인각을 찾기 위한 3개 point의 index set\n",
    "angle_triples = (\n",
    "    (0, 1, 2), # thumb\n",
    "    (1, 2, 3),\n",
    "    (2, 3, 4),\n",
    "\n",
    "    (0, 5, 6), # index\n",
    "    (5, 6, 7),\n",
    "    (6, 7, 8),\n",
    "\n",
    "    (0, 9, 10), # middle\n",
    "    (9, 10, 11),\n",
    "    (10, 11, 12),\n",
    "\n",
    "    (0, 13, 14), # ring\n",
    "    (13, 14, 15),\n",
    "    (14, 15, 16),\n",
    "\n",
    "    (0, 17, 18), # little\n",
    "    (17, 18, 19),\n",
    "    (18, 19, 20),\n",
    "\n",
    "    (1, 0, 5),\n",
    "    (17, 0, 5),\n",
    "    (0, 5, 9),\n",
    "    (5, 9, 13),\n",
    "    (9, 13, 17),\n",
    "    (13, 17, 0),\n",
    "\n",
    "    (6, 5, 9),\n",
    "    (5, 9, 10),\n",
    "    (10, 9, 13),\n",
    "    (9, 13, 14),\n",
    "    (14, 13, 17),\n",
    "    (13, 17, 18)\n",
    ")\n",
    "\n",
    "def gen_triples(array):\n",
    "    \"\"\"\n",
    "    위 3개 point index를 활용하여 array로부터 3개 point set을 반환하는 generator\n",
    "    \"\"\"\n",
    "    for idxs in angle_triples:\n",
    "        v1 = array[idxs[0]]\n",
    "        v2 = array[idxs[1]]\n",
    "        v3 = array[idxs[2]]\n",
    "        yield v1, v2, v3\n",
    "\n",
    "\n",
    "def cos_of_triples(array):\n",
    "    \"\"\"\n",
    "    21개 행을 갖는 keypoint array의 cosine list를 반환\n",
    "    \"\"\"\n",
    "    return list(map(lambda x: cos_vec3(*x), gen_triples(array)))\n",
    "\n",
    "def sin_of_triples(array):\n",
    "    \"\"\"\n",
    "    21개 행을 갖는 keypoint array의 sine list를 반환\n",
    "    \"\"\"\n",
    "    return list(map(lambda x: clockwise_sin2d_vec3(*x), gen_triples(array)))\n",
    "\n",
    "\n",
    "def pairwise_diff(array):\n",
    "    \"\"\"\n",
    "    21개 keypoint의 모든 pair에 대하여 distance를 산출\n",
    "    21C2 = 210개의 comibation\n",
    "    \"\"\"\n",
    "    return np.concatenate(list(map(lambda x: x[0]-x[1], combinations(array, 2))), 0)\n",
    "\n",
    "\n",
    "def preprocess_file_info(file_info, image_dir, split, label_info_mapping, blob_detector, flip=False):\n",
    "    rows = []\n",
    "    row_file = {}\n",
    "\n",
    "    row_file[\"id\"] = file_info[\"id\"]\n",
    "    row_file[\"n_img\"] = len(file_info[\"annotations\"])\n",
    "    row_file[\"flip\"] = flip\n",
    "\n",
    "    # All frame freature\n",
    "    all_frame_ann_array = np.array([kps[\"data\"] for kps in file_info[\"annotations\"]])\n",
    "\n",
    "    if flip:\n",
    "        all_frame_ann_array[:, :, 0] = 1920 - all_frame_ann_array[:, :, 0]\n",
    "    if row_file[\"id\"] == 475:\n",
    "        all_frame_keypoint_array = all_frame_ann_array[:, 21:]\n",
    "    elif row_file[\"id\"] == 543:\n",
    "        all_frame_keypoint_array = all_frame_ann_array[:, :21]\n",
    "\n",
    "    all_frame_keypoint_array = all_frame_ann_array.reshape(all_frame_ann_array.shape[0], -1, 21, 3)\n",
    "    file_all_frame_one_box = np.c_[all_frame_keypoint_array.min(2), all_frame_keypoint_array.max(2)]\n",
    "\n",
    "    all_frame_palm_width = np.linalg.norm(all_frame_keypoint_array[:, :, [5, 9, 13], :2] - all_frame_keypoint_array[:, :, [9, 13, 17], :2], axis=-1)\n",
    "    pw_max_diff = (all_frame_palm_width.max(0) - all_frame_palm_width.min(0)).mean(-1)\n",
    "    file_palm_height = np.linalg.norm(all_frame_keypoint_array[:, :, 0, :2] - all_frame_keypoint_array[:, :, 5, :2], axis=-1, keepdims=True)\n",
    "    file_index_move_mean = (all_frame_keypoint_array[:, :, 8, :2] - all_frame_keypoint_array[:, :, 0, :2]) / file_palm_height\n",
    "\n",
    "    if split == \"train\":\n",
    "        # Split이 train인 경우 label 정보 추가\n",
    "        row_file[\"pose_id\"] = file_info[\"action\"][0]\n",
    "        if row_file[\"id\"] == 282: # 282의 mis-label\n",
    "            row_file[\"pose_id\"] = 74 # Fix mis-label, 약속1 -> 약속2\n",
    "        row_file.update(label_info_mapping[row_file[\"pose_id\"]])\n",
    "        if flip:\n",
    "            if row_file[\"hand_type\"] == \"left\":\n",
    "                row_file[\"hand_type\"] = \"right\"\n",
    "            elif row_file[\"hand_type\"] == \"right\":\n",
    "                row_file[\"hand_type\"] = \"left\"\n",
    "    for bbox_idx in range(all_frame_keypoint_array.shape[1]):\n",
    "        row_bbox = {\"bbox_num\": bbox_idx}\n",
    "        row_bbox.update(row_file)\n",
    "        row_bbox[\"pw_max_diff\"] = pw_max_diff[bbox_idx]\n",
    "        row_bbox[\"index_move_mean\"] = pdist(file_index_move_mean[:, bbox_idx]).mean()\n",
    "        row_bbox[\"index_move_x_mean\"] = pdist(file_index_move_mean[:, bbox_idx, :1]).mean()\n",
    "        all_frame_one_box = file_all_frame_one_box[:, bbox_idx]\n",
    "\n",
    "        for keypoint_idx in range(row_file[\"n_img\"]):\n",
    "            row = {}\n",
    "            row[\"img_num\"] = keypoint_idx\n",
    "            row.update(row_bbox)\n",
    "            ann_array = all_frame_keypoint_array[keypoint_idx].reshape(-1, 3)\n",
    "\n",
    "            keypoint_array = all_frame_keypoint_array[keypoint_idx, bbox_idx]\n",
    "\n",
    "            image_path = os.path.join(image_dir, f\"{row['id']}/{keypoint_idx}.png\")\n",
    "            if \"n_blob_keypoints\" not in row_file:\n",
    "                row_file[\"n_blob_keypoints\"] = get_num_keypoint(image_path, blob_detector)\n",
    "                row_file[\"n_hands\"] = \"single\" if row_file[\"n_blob_keypoints\"] < 23 else \"both\" # 한손의 경우 21개보다 많은 경우는 없지만 여유롭게 잡음\n",
    "                row_bbox[\"n_blob_keypoints\"] = row_file[\"n_blob_keypoints\"]\n",
    "                row_bbox[\"n_hands\"] = row_file[\"n_hands\"]\n",
    "                row[\"n_blob_keypoints\"] = row_file[\"n_blob_keypoints\"]\n",
    "                row[\"n_hands\"] = row_file[\"n_hands\"]\n",
    "\n",
    "            # 한 손 bbox의 너비와 높이\n",
    "            one_bbox = all_frame_one_box[keypoint_idx]\n",
    "            for i, b in enumerate(one_bbox):\n",
    "                row[f\"one_bbox_{i}\"] = b\n",
    "            row[\"one_bbox_w\"] = one_bbox[3] - one_bbox[0]\n",
    "            row[\"one_bbox_h\"] = one_bbox[4] - one_bbox[1]\n",
    "\n",
    "            # 양손 bbox의 너비와 높이\n",
    "            bbox = np.r_[ann_array.min(0), ann_array.max(0)]\n",
    "            for i, b in enumerate(bbox):\n",
    "                row[f\"bbox_{i}\"] = b\n",
    "            row[\"bbox_w\"] = bbox[3] - bbox[0]\n",
    "            row[\"bbox_h\"] = bbox[4] - bbox[1]\n",
    "\n",
    "            # 여러 3개쌍 key point에 대한 cos / sin 값\n",
    "            coses = cos_of_triples(keypoint_array[:, :2])\n",
    "            for i, cos_anlge in enumerate(coses):\n",
    "                row[f\"cos_{i}\"] = cos_anlge\n",
    "            sins = sin_of_triples(keypoint_array[:, :2])\n",
    "            for i, sin_angle in enumerate(sins):\n",
    "                row[f\"sin_{i}\"] = sin_angle\n",
    "            for i, sin_angle in enumerate(sins):\n",
    "                row[f\"sin_abs_{i}\"] = abs(sin_angle)\n",
    "\n",
    "            # 각 point의 pairwise diff와 distance를 normalization하기 위한 손바닥 높이\n",
    "            # 2d에 대한 scale이 성능이 좋아보임\n",
    "            palm_height = np.linalg.norm(keypoint_array[0, :2] - keypoint_array[5, :2])\n",
    "\n",
    "            # 모든 point 쌍에 대한 차\n",
    "            # 3차원이기에 pair 수 210 * 3 = 630개\n",
    "            diffs = pairwise_diff(keypoint_array) / palm_height\n",
    "            for i, diff in enumerate(diffs):\n",
    "                row[f\"diff_{i}\"] = diff\n",
    "\n",
    "            # 모든 point 쌍에 대한 Euclidean distance\n",
    "            # 2차원 distance보다 3차원 distance가 성능상 좋았음\n",
    "            dists = pdist(keypoint_array) / palm_height\n",
    "            for i, dist in enumerate(dists):\n",
    "                row[f\"dist_{i}\"] = dist\n",
    "\n",
    "            thumb_knuckle = keypoint_array[4, :2] - keypoint_array[3, :2]\n",
    "            index_knuckle = keypoint_array[8, :2] - keypoint_array[7, :2]\n",
    "            palm_knuckle = keypoint_array[13, :2] - keypoint_array[17, :2]\n",
    "\n",
    "            row[\"thumb_index_cos\"] = cos(thumb_knuckle, index_knuckle)\n",
    "            row[\"thumb_palm_cos\"] = cos(thumb_knuckle, palm_knuckle)\n",
    "            row[\"dist_index\"] = np.linalg.norm(keypoint_array[8, :2] - keypoint_array[5, :2]) / palm_height\n",
    "            row[\"dist_middle\"] = np.linalg.norm(keypoint_array[12, :2] - keypoint_array[9, :2]) / palm_height\n",
    "            row[\"dist_ring\"] = np.linalg.norm(keypoint_array[16, :2] - keypoint_array[13, :2]) / palm_height\n",
    "            row[\"dist_little\"] = np.linalg.norm(keypoint_array[20, :2] - keypoint_array[17, :2]) / palm_height\n",
    "\n",
    "            row[\"fold_thumb\"] = row[\"thumb_palm_cos\"] < 0.2\n",
    "            row[\"fold_index\"] = row[\"dist_index\"] < 0.5\n",
    "            row[\"fold_middle\"] = row[\"dist_middle\"] < 0.5\n",
    "            row[\"fold_ring\"] = row[\"dist_ring\"] < 0.5\n",
    "            row[\"fold_little\"] = row[\"dist_little\"] < 0.5\n",
    "\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def data2df(data_path, image_dir, split, label_info_mapping, csv_path=None):\n",
    "    \"\"\"\n",
    "    각 split의 keypoint 및 label 데이터를 처리하여 csv로 저장하고 data frame을 반환\n",
    "    \"\"\"\n",
    "    data_list = os.listdir(data_path)\n",
    "    image_dir = os.path.join(image_dir, split)\n",
    "    blob_detector = make_blob_detector()\n",
    "\n",
    "    print(\"Preprocessing\")\n",
    "    rows = []\n",
    "    for file_name in tqdm(data_list, split):\n",
    "        json_path = os.path.join(data_path, file_name)\n",
    "        with open(json_path) as f:\n",
    "            file_info = json.load(f)\n",
    "        frame_rows = preprocess_file_info(file_info, image_dir, split, label_info_mapping, blob_detector, False)\n",
    "        flip_frame_row = preprocess_file_info(file_info, image_dir, split, label_info_mapping, blob_detector, True)\n",
    "        if frame_rows[0][\"id\"] in (490, 586, 596, 613):\n",
    "            continue\n",
    "        rows.extend(frame_rows)\n",
    "        rows.extend(flip_frame_row)\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values(\"id\").reset_index(drop=True)\n",
    "    if csv_path is not None:\n",
    "        df.to_csv(csv_path, index=False, encoding=\"cp949\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe7cab7",
   "metadata": {},
   "source": [
    "### 데이터 처리\n",
    "- 숫자의 경우 **숫자 1**, **숫자1**과 같이 공백 여부로 나뉨. 이를 동일하게 인식하도록 공백 제거\n",
    "- 동그라미의 경우 양손과 한손의 이미지가 상당히 다른 패턴임. 이를 다르게 인식하도록 수정\n",
    "- 손하트의 경우 my hand와 your hand의 패턴이 상당히 다름. 이를 다르게 인식하도록 수정\n",
    "- 약속의 경우 엄지손가락을 접은 경우, 접지 않은 경우로 나뉨. 이를 나누도록 수정 (참고: https://dacon.io/competitions/official/235805/support/404582)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c927845",
   "metadata": {
    "id": "7c927845",
    "outputId": "4768913f-fc02-49fe-b159-6c5f9d92ad6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/workspace/LightGBM + CLIP.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646565706f5f6a68222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e37342e3832227d7d/home/workspace/LightGBM%20%2B%20CLIP.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m train_data_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_path, \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646565706f5f6a68222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e37342e3832227d7d/home/workspace/LightGBM%20%2B%20CLIP.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m train_new_json_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(new_json_path, \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646565706f5f6a68222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e37342e3832227d7d/home/workspace/LightGBM%20%2B%20CLIP.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m train_df \u001b[39m=\u001b[39m data2df(train_new_json_path, data_path, \u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m, label_info_mappnig, \u001b[39m\"\u001b[39;49m\u001b[39mtrain_annotation_frame.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646565706f5f6a68222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e37342e3832227d7d/home/workspace/LightGBM%20%2B%20CLIP.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# train_df = data2df(train_data_path, data_path, \"train\", label_info_mappnig, \"train_annotation_frame.csv\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646565706f5f6a68222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e37342e3832227d7d/home/workspace/LightGBM%20%2B%20CLIP.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m test_data_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_path, \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/workspace/LightGBM + CLIP.ipynb Cell 7\u001b[0m in \u001b[0;36mdata2df\u001b[0;34m(data_path, image_dir, split, label_info_mapping, csv_path)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646565706f5f6a68222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e37342e3832227d7d/home/workspace/LightGBM%20%2B%20CLIP.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=272'>273</a>\u001b[0m     rows\u001b[39m.\u001b[39mextend(frame_rows)\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646565706f5f6a68222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e37342e3832227d7d/home/workspace/LightGBM%20%2B%20CLIP.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=273'>274</a>\u001b[0m     rows\u001b[39m.\u001b[39mextend(flip_frame_row)\n\u001b[0;32m--> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646565706f5f6a68222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e37342e3832227d7d/home/workspace/LightGBM%20%2B%20CLIP.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=275'>276</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(rows)\u001b[39m.\u001b[39;49msort_values(\u001b[39m\"\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646565706f5f6a68222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e37342e3832227d7d/home/workspace/LightGBM%20%2B%20CLIP.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=276'>277</a>\u001b[0m \u001b[39mif\u001b[39;00m csv_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646565706f5f6a68222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e37342e3832227d7d/home/workspace/LightGBM%20%2B%20CLIP.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=277'>278</a>\u001b[0m     df\u001b[39m.\u001b[39mto_csv(csv_path, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcp949\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:6307\u001b[0m, in \u001b[0;36mDataFrame.sort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   6303\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(by):\n\u001b[1;32m   6304\u001b[0m     \u001b[39m# len(by) == 1\u001b[39;00m\n\u001b[1;32m   6306\u001b[0m     by \u001b[39m=\u001b[39m by[\u001b[39m0\u001b[39m]\n\u001b[0;32m-> 6307\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_label_or_level_values(by, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m   6309\u001b[0m     \u001b[39m# need to rewrap column in Series to apply key function\u001b[39;00m\n\u001b[1;32m   6310\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   6311\u001b[0m         \u001b[39m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[1;32m   6312\u001b[0m         \u001b[39m# \"Series\", variable has type \"ndarray\")\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py:1848\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis]\u001b[39m.\u001b[39mget_level_values(key)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   1847\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1848\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n\u001b[1;32m   1850\u001b[0m \u001b[39m# Check for duplicates\u001b[39;00m\n\u001b[1;32m   1851\u001b[0m \u001b[39mif\u001b[39;00m values\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "data_path = \"/home/workspace/handpose_data/\"\n",
    "new_json_path = os.path.join(data_path, \"new_jsons\")\n",
    "\n",
    "submission_test_path = \".\"\n",
    "os.makedirs(submission_test_path, exist_ok=True)\n",
    "os.chdir(submission_test_path)\n",
    "\n",
    "\"\"\"\n",
    "pose_name 정보를 좀더 구체적으로 수정함\n",
    "1. 약속 class를 엄지의 위치에 따라 약속1, 약속2로 나눔\n",
    "2. '숫자 1', '숫자1'과 같이 숫자 클래스가 공백으로 나누어져있기에 이를 병합\n",
    "3. 동그라미가 한손일 때와 양손일 때의 패턴의 차이가 크기에 이를 나눔\n",
    "4. 손하트가 my_hand와 your_hand의 차이가 크기에 이를 나눔\n",
    "=> pose_name 종류의 수가 41개로 바뀜\n",
    "\"\"\"\n",
    "label_info = pd.read_csv(os.path.join(data_path, \"hand_gesture_pose.csv\"))\n",
    "label_info.pose_name = label_info.pose_name.str.replace(\"숫자 \", \"숫자\")\n",
    "\n",
    "label_info.loc[label_info.pose_id == 29, \"pose_name\"] = \"약속1\"\n",
    "label_info.loc[label_info.pose_id == 54, \"pose_name\"] = \"약속1\"\n",
    "label_info.loc[label_info.pose_id == 79, \"pose_name\"] = \"약속1\"\n",
    "label_info.loc[label_info.pose_id == 129, \"pose_name\"] = \"약속1\"\n",
    "label_info.loc[label_info.pose_id == 154, \"pose_name\"] = \"약속1\"\n",
    "label_info.loc[label_info.pose_id == 49, \"pose_name\"] = \"약속2\"\n",
    "label_info.loc[label_info.pose_id == 74, \"pose_name\"] = \"약속2\"\n",
    "label_info.loc[label_info.pose_id == 124, \"pose_name\"] = \"약속2\"\n",
    "label_info.loc[label_info.pose_id == 149, \"pose_name\"] = \"약속2\"\n",
    "label_info.loc[label_info.pose_id == 174, \"pose_name\"] = \"약속2\"\n",
    "\n",
    "\n",
    "label_info.loc[label_info.pose_id == 90, \"pose_name\"] = \"동그라미-양손\"\n",
    "label_info.loc[label_info.pose_id == 190, \"pose_name\"] = \"동그라미-양손\"\n",
    "label_info.loc[label_info.pose_id == 145, \"pose_name\"] = \"손하트-yourhand\"\n",
    "assert label_info.pose_name.value_counts().shape[0] == 41 # 수정한 pose_name를 확인\n",
    "\n",
    "label_info_mappnig = {row.pop(\"pose_id\"): row for row in label_info.to_dict(\"records\")}\n",
    "\n",
    "train_data_path = os.path.join(data_path, \"train\")\n",
    "train_new_json_path = os.path.join(new_json_path, \"train\")\n",
    "train_df = data2df(train_new_json_path, data_path, \"train\", label_info_mappnig, \"train_annotation_frame.csv\")\n",
    "# train_df = data2df(train_data_path, data_path, \"train\", label_info_mappnig, \"train_annotation_frame.csv\")\n",
    "\n",
    "test_data_path = os.path.join(data_path, \"test\")\n",
    "test_new_json_path = os.path.join(new_json_path, \"test\")\n",
    "test_df = data2df(test_new_json_path, data_path, \"test\", label_info_mappnig, \"test_annotation_frame.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4f2129",
   "metadata": {
    "id": "6f4f2129"
   },
   "source": [
    "## Bbox feature extraction\n",
    "\n",
    "Bounding box feature를 [CLIP](https://github.com/openai/CLIP)으로 embedding하여 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bffd8c5b",
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1634458681626,
     "user": {
      "displayName": "Jaesun Park",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgZoXwLcDlJDxjDM_dDPkJjpEemuLU0RkMAa3ImbQU=s64",
      "userId": "10621921894899269106"
     },
     "user_tz": -540
    },
    "id": "bffd8c5b"
   },
   "outputs": [],
   "source": [
    "def process_bbox(image, bbox_xyxy):\n",
    "    \"\"\"\n",
    "    1. bbox 확장 방법\n",
    "        Fixed pixel 확장: 100픽셀 확장\n",
    "    2. bbox clipping: bbox가 이미지 영역을 넘어갈 때 처리 방법\n",
    "        zero-padded bbox\n",
    "    \"\"\"\n",
    "    thr = 100\n",
    "    bbox_xyxy[0] -= thr\n",
    "    bbox_xyxy[2] += thr\n",
    "    bbox_xyxy[1] -= thr\n",
    "    bbox_xyxy[3] += thr\n",
    "    new_xyxy = bbox_xyxy\n",
    "    image = image.crop(new_xyxy)\n",
    "    return image\n",
    "\n",
    "\n",
    "class CsvDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Preprocessing을 거쳐 만들어진 csv 파일을 활용해 CLIP에 사용할 Dataset 생성\n",
    "    \"\"\"\n",
    "    def __init__(self, image_root, csv_path, transform=None) -> None:\n",
    "        super().__init__()\n",
    "        self.image_root = image_root\n",
    "        self.transform = transform\n",
    "        self.data_info = pd.read_csv(csv_path, encoding=\"cp949\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data_info.iloc[index]\n",
    "        file_id = int(item.id)\n",
    "        img_num = item.img_num\n",
    "        img_path = os.path.join(self.image_root, f\"{file_id}/{img_num}.png\")\n",
    "        image = pil_loader(img_path)\n",
    "        if item.flip:\n",
    "            image = image.transpose(method=Image.FLIP_LEFT_RIGHT)\n",
    "        bbox_xyxy = np.array([item.one_bbox_0, item.one_bbox_1, item.one_bbox_3, item.one_bbox_4])\n",
    "        image = process_bbox(image, bbox_xyxy)\n",
    "        image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_info.shape[0]\n",
    "\n",
    "\n",
    "def get_features(dataset):\n",
    "    \"\"\"\n",
    "    데이터 저장을 위한 CLIP feature 생성\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(DataLoader(dataset, batch_size=100, num_workers=0)):\n",
    "            features = model.encode_image(images.to(device))\n",
    "            all_features.append(features)\n",
    "    return torch.cat(all_features).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eceed98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1+cu102'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39a4351e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39a4351e",
    "outputId": "f9d15142-bee2-488e-bde3-874eb1cb354e"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_annotation_frame.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/workspace/LightGBM + CLIP.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646565706f5f6a68222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e37342e3832227d7d/home/workspace/LightGBM%20%2B%20CLIP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646565706f5f6a68222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e37342e3832227d7d/home/workspace/LightGBM%20%2B%20CLIP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model, preprocess \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mViT-B/32\u001b[39m\u001b[39m'\u001b[39m, device)\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646565706f5f6a68222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e37342e3832227d7d/home/workspace/LightGBM%20%2B%20CLIP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m train \u001b[39m=\u001b[39m CsvDataset(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(data_path, \u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m), \u001b[39m\"\u001b[39;49m\u001b[39mtrain_annotation_frame.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, preprocess)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646565706f5f6a68222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e37342e3832227d7d/home/workspace/LightGBM%20%2B%20CLIP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m test \u001b[39m=\u001b[39m CsvDataset(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_path, \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mtest_annotation_frame.csv\u001b[39m\u001b[39m\"\u001b[39m, preprocess)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646565706f5f6a68222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e37342e3832227d7d/home/workspace/LightGBM%20%2B%20CLIP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Calculate the image features\u001b[39;00m\n",
      "\u001b[1;32m/home/workspace/LightGBM + CLIP.ipynb Cell 11\u001b[0m in \u001b[0;36mCsvDataset.__init__\u001b[0;34m(self, image_root, csv_path, transform)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646565706f5f6a68222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e37342e3832227d7d/home/workspace/LightGBM%20%2B%20CLIP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_root \u001b[39m=\u001b[39m image_root\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646565706f5f6a68222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e37342e3832227d7d/home/workspace/LightGBM%20%2B%20CLIP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39m=\u001b[39m transform\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646565706f5f6a68222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3136332e3135322e37342e3832227d7d/home/workspace/LightGBM%20%2B%20CLIP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_info \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(csv_path, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcp949\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m     f,\n\u001b[1;32m   1219\u001b[0m     mode,\n\u001b[1;32m   1220\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1221\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1222\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1223\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1224\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1225\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1226\u001b[0m )\n\u001b[1;32m   1227\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    790\u001b[0m             handle,\n\u001b[1;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    792\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    793\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    794\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    795\u001b[0m         )\n\u001b[1;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_annotation_frame.csv'"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "data_path = \"/home/workspace/handpose_data/\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "train = CsvDataset(os.path.join(data_path, \"train\"), \"train_annotation_frame.csv\", preprocess)\n",
    "test = CsvDataset(os.path.join(data_path, \"test\"), \"test_annotation_frame.csv\", preprocess)\n",
    "\n",
    "# Calculate the image features\n",
    "\n",
    "train_features = get_features(train)\n",
    "train_clip_features = pd.DataFrame(train_features)\n",
    "train_clip_features.columns = [f\"clip_bbox_feature_{i}\" for i in range(train_features.shape[1])]\n",
    "pd.concat((train.data_info.id, train.data_info.img_num, train.data_info.bbox_num, train.data_info.flip, train_clip_features), axis=1).to_csv(\"train_annotation_clip_bbox100_frame.csv\", index=False)\n",
    "\n",
    "test_features = get_features(test)\n",
    "test_clip_features = pd.DataFrame(test_features)\n",
    "test_clip_features.columns = [f\"clip_bbox_feature_{i}\" for i in range(test_features.shape[1])]\n",
    "pd.concat((test.data_info.id, test.data_info.img_num, test.data_info.bbox_num, test.data_info.flip, test_clip_features), axis=1).to_csv(\"test_annotation_clip_bbox100_frame.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9992c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotation_path = \"train_annotation_frame.csv\"\n",
    "train_annotation_clip_bbox_path = \"train_annotation_clip_bbox100_frame.csv\"\n",
    "\n",
    "data_test_path = os.path.join(data_path, \"test\")\n",
    "test_annotation_path = \"test_annotation_frame.csv\"\n",
    "test_annotation_clip_bbox_path = \"test_annotation_clip_bbox100_frame.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_annotation_path, encoding=\"cp949\")\n",
    "train_df_clip_bbox = pd.read_csv(train_annotation_clip_bbox_path)\n",
    "\n",
    "test_df = pd.read_csv(test_annotation_path, encoding=\"cp949\")\n",
    "test_df_clip_bbox = pd.read_csv(test_annotation_clip_bbox_path)\n",
    "\n",
    "train_df = train_df.merge(train_df_clip_bbox, on=[\"id\", \"img_num\", \"bbox_num\", \"flip\"])\n",
    "test_df = test_df.merge(test_df_clip_bbox, on=[\"id\", \"img_num\", \"bbox_num\", \"flip\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "015585a9",
   "metadata": {
    "id": "015585a9"
   },
   "outputs": [],
   "source": [
    "train_df[\"n_hands\"] = pd.Categorical(train_df[\"n_hands\"])\n",
    "test_df[\"n_hands\"] = pd.Categorical(test_df[\"n_hands\"])\n",
    "\n",
    "features = \\\n",
    "    [f\"diff_{i}\" for i in range(630)] + \\\n",
    "    [f\"dist_{i}\" for i in range(210)] + \\\n",
    "    [f\"cos_{i}\" for i in range(27)] + \\\n",
    "    [f\"sin_{i}\" for i in range(27)] + \\\n",
    "    [f\"sin_abs_{i}\" for i in range(27)] + \\\n",
    "    [f\"clip_bbox_feature_{i}\" for i in range(512)] + \\\n",
    "    [\"n_blob_keypoints\", \"n_hands\", \"bbox_w\", \"bbox_h\", \"one_bbox_w\", \"one_bbox_h\"] + \\\n",
    "    [\"thumb_index_cos\", \"thumb_palm_cos\", \"fold_thumb\", \"fold_index\", \"fold_middle\", \"fold_ring\", \"fold_little\"] + \\\n",
    "    [\"pw_max_diff\", \"index_move_mean\", \"index_move_x_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63610d85",
   "metadata": {
    "id": "63610d85"
   },
   "source": [
    "## Train model\n",
    "\n",
    "Light GBM으로 학습하고 예측\n",
    "\n",
    "`lgb.train`의 seed를 고정하지 않아 제출한 결과와 약간 다를 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87c602c2",
   "metadata": {
    "id": "87c602c2"
   },
   "outputs": [],
   "source": [
    "def train_model(train_df, test_df, features, target_col, path=\"log\", add_params={}):\n",
    "    print(\"Start training\")\n",
    "    print(f\"Target: {target_col}\")\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    train_df[target_col] = pd.Categorical(train_df[target_col])\n",
    "\n",
    "    x_train = train_df[features]\n",
    "    y_train = train_df[target_col].cat.codes\n",
    "    num_class = len(set(y_train))\n",
    "\n",
    "    dtrain = lgb.Dataset(x_train, label=y_train)\n",
    "\n",
    "    x_test = test_df[features]\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"multiclass\",\n",
    "        \"metric\": \"multi_logloss\",\n",
    "        \"num_class\": num_class,\n",
    "        \"verbosity\": -1,\n",
    "    }\n",
    "    params.update(add_params)\n",
    "    train_df[f\"pred_{target_col}\"] = pd.Categorical(train_df[target_col])\n",
    "\n",
    "    model = lgb.train(params, dtrain)\n",
    "    result = model.predict(x_test)\n",
    "    test_df[f\"pred_{target_col}\"] = pd.Categorical(result.argmax(-1))\n",
    "\n",
    "    if target_col == \"hand_type\":\n",
    "        new_df = pd.concat((test_df[[\"id\"]], pd.DataFrame(result)), axis=1)\n",
    "        flip_temp = new_df[test_df.flip].copy()\n",
    "        new_df.loc[test_df.flip, 1] = flip_temp.loc[:, 2]\n",
    "        new_df.loc[test_df.flip, 2] = flip_temp.loc[:, 1]\n",
    "    else:\n",
    "        new_df = pd.concat((test_df[[\"id\"]], pd.DataFrame(result)), axis=1)\n",
    "    merged_df = new_df.groupby(\"id\", as_index=False).mean()\n",
    "    result = merged_df.iloc[:, 1:(1+num_class)].to_numpy()\n",
    "    y_pred = result.argmax(-1)\n",
    "\n",
    "    result_df = pd.concat((merged_df.id, pd.DataFrame(np.concatenate((y_pred[:, None], result), 1))), ignore_index=True, axis=1)\n",
    "    result_df.columns = [\"id\", f\"pred_{target_col}\"] + train_df[target_col].cat.categories.to_list()\n",
    "\n",
    "    result_df.to_csv(f\"{path}/result_test_{target_col}.csv\", index=False)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59618b58",
   "metadata": {},
   "source": [
    "Optuna로 탐색한 Hyperparameter 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5218423e",
   "metadata": {
    "id": "5218423e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Target: gesture_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "1it [00:04,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Target: hand_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:10,  5.63s/it]C:\\Users\\dilig\\.conda\\envs\\ego-vision\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Target: pose_name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [19:49, 396.37s/it]\n"
     ]
    }
   ],
   "source": [
    "save_path = \"submission\"\n",
    "targets = (\"gesture_type\", \"hand_type\", \"pose_name\")\n",
    "train_add_params = [\n",
    "    {\n",
    "        \"feature_pre_filter\": False,\n",
    "        \"lambda_l1\": 1.7657137779105168e-06,\n",
    "        \"lambda_l2\": 2.3530332427385596e-06,\n",
    "        \"num_leaves\": 6,\n",
    "        \"feature_fraction\": 0.4,\n",
    "        \"bagging_fraction\": 0.6541928796037666,\n",
    "        \"bagging_freq\": 5,\n",
    "        \"min_child_samples\": 20\n",
    "    },\n",
    "    {\n",
    "        \"feature_pre_filter\": False,\n",
    "        \"lambda_l1\": 1.5204270129130175e-08,\n",
    "        \"lambda_l2\": 0.31158648353398066,\n",
    "        \"num_leaves\": 140,\n",
    "        \"feature_fraction\": 0.41600000000000004,\n",
    "        \"bagging_fraction\": 0.7616580256435892,\n",
    "        \"bagging_freq\": 4,\n",
    "        \"min_child_samples\": 20\n",
    "    },\n",
    "    {\n",
    "        \"num_boost_round\": 10000,\n",
    "        \"feature_pre_filter\": False,\n",
    "        \"min_data_in_leaf\": 100,\n",
    "        \"lambda_l1\": 8.685219254418955e-07,\n",
    "        \"lambda_l2\": 8.519494831720772,\n",
    "        \"num_leaves\": 141,\n",
    "        \"feature_fraction\": 0.41600000000000004,\n",
    "        \"bagging_fraction\": 0.44262163491880324,\n",
    "        \"bagging_freq\": 1,\n",
    "    }\n",
    "]\n",
    "\n",
    "result_dfs = {}\n",
    "for target_col, add_params in tqdm(zip(targets, train_add_params)):\n",
    "    result_dfs[target_col] = train_model(train_df, test_df, features, target_col, save_path, add_params)\n",
    "    features.append(f\"pred_{target_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f863d96",
   "metadata": {
    "id": "7f863d96"
   },
   "outputs": [],
   "source": [
    "for target_col in (\"hand_type\", \"gesture_type\"):\n",
    "    probs = result_dfs[target_col].iloc[:, 2:].to_numpy()\n",
    "    one_hot = np.zeros_like(probs)\n",
    "    for i, p in enumerate(probs.argmax(-1)):\n",
    "        one_hot[i, p] = 1.\n",
    "    result_dfs[target_col].iloc[:, 2:] = one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "699c3ded",
   "metadata": {
    "id": "699c3ded"
   },
   "outputs": [],
   "source": [
    "sample_submision_path = os.path.join(data_path, \"sample_submission.csv\")\n",
    "submission = pd.read_csv(sample_submision_path)\n",
    "for row in label_info.to_dict(\"records\"):\n",
    "    col_prob = np.ones(result_dfs[target_col].shape[0], dtype=float)\n",
    "    for target_col in targets:\n",
    "        col_prob *= result_dfs[target_col][row[target_col]]\n",
    "    submission[f\"Label_{row['pose_id']}\"] = col_prob\n",
    "submission.iloc[:, 1:] = submission.iloc[:, 1:].to_numpy() / np.sum(submission.iloc[:, 1:].to_numpy(), axis=1, keepdims=True)\n",
    "submission.to_csv(f\"{save_path}/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "submission.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
